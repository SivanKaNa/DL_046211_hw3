{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SivanKaNa/DL_046211_hw3/blob/master/ece046211_hw3_seq_tasks_students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzV9wsJ5pGhf"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/bubbles/50/000000/mind-map.png\" style=\"height:50px;display:inline\"> ECE 046211 - Technion - Deep Learning\n",
        "---\n",
        "\n",
        "## HW3 - Sequential Tasks and Training Methods\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq2c8X93pGhh"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
        "---\n",
        "* Run current cell: **Ctrl + Enter**\n",
        "* Run current cell and move to the next: **Shift + Enter**\n",
        "* Show lines in a code cell: **Esc + L**\n",
        "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
        "* New cell below: **Esc + B**\n",
        "* Delete cell: **Esc + D, D** (two D's)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZZybn3NpGhh"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
        "---\n",
        "* Fill in\n",
        "\n",
        "|Name     |Campus Email| ID  |\n",
        "|---------|--------------------------------|----------|\n",
        "|Student 1| student_1@campus.technion.ac.il| 123456789|\n",
        "|Student 2| student_2@campus.technion.ac.il| 987654321|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDK5zqhdpGhi"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
        "---\n",
        "* Maximal garde: 100.\n",
        "* Submission only in **pairs**.\n",
        "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
        "* **No handwritten submissions.** You can choose whether to answer in a Markdown cell in this notebook or attach a PDF with your answers.\n",
        "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
        "* What you have to submit:\n",
        "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ece046211_hw3_id1_id2.ipynb`.\n",
        "    * If you answered the questionss in a different file you should submit a `.zip` file with the name `ece046211_hw3_id1_id2.zip` with content:\n",
        "        * `ece046211_hw3_id1_id2.ipynb` - the code tasks\n",
        "        * `ece046211_hw3_id1_id2.pdf` - answers to questions.\n",
        "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
        "* Submission on the course website (Moodle).\n",
        "* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers (\"* some text here with Latex equations\" -> \"some text here with Latex equations\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmSj_UufpGhi"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/dusk/64/000000/online.png\" style=\"height:50px;display:inline\"> Working Online and Locally\n",
        "---\n",
        "* You can choose your working environment:\n",
        "    1. `Jupyter Notebook`, **locally** with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or **online** on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
        "        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\\rightarrow$ `Change Runtime Type` $\\rightarrow$`GPU`.\n",
        "    2. Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
        "        * Both allow editing and running Jupyter Notebooks.\n",
        "\n",
        "* Please refer to `Setting Up the Working Environment.pdf` on the Moodle or our GitHub (https://github.com/taldatech/ee046211-deep-learning) to help you get everything installed.\n",
        "* If you need any technical assistance, please go to our Piazza forum (`hw3` folder) and describe your problem (preferably with images)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlp1Fp4ppGhj"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
        "---\n",
        "\n",
        "* [Part 1 - Theory](#-Part-1---Theory)\n",
        "    * [Q1 - Dropout](#-Question-1--Dropout)\n",
        "    * [Q2 - Preventing Variance Explosion](#-Question-2--Preventing-Variance-Explosion)\n",
        "    * [Q3 - Batch Normalization](#-Solution-4--Batch-Normalization)\n",
        "* [Part 2 - Code Assignments - Sequence-to-Sequence with Transformers](#-Part-2---Code-Assignments)\n",
        "    * [Task 1 - Task 1 - Loading and Observing the Data](#-Task-1----Loading-and-Observing-the-Data)\n",
        "    * [Task 2 - Preparing the Data - Separating to Inputs and Targets](#-Task-2----Preparing-the--Data---Separating-to-Inputs-and-Targets)\n",
        "    * [Task 3 - Define Hyperparameters and Initialize the Model](#-Task-3----Define-Hyperparameters-and-Initialize-the-Model)\n",
        "    * [Task 4 - Train and Evaluate the Language Model](#-Task-4----Train-and-Evaluate-the-Language-Model)\n",
        "    * [Task 5 - Generate Sentences](#-Task-5----Generate-Sentences)\n",
        "* [Credits](#-Credits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKtSiQX_pGhj"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/ball-point-pen.png\" style=\"height:50px;display:inline\"> Part 1 - Theory\n",
        "---\n",
        "* You can choose whether to answser these straight in the notebook (Markdown + Latex) or use another editor (Word, LyX, Latex, Overleaf...) and submit an additional PDF file, **but no handwritten submissions**.\n",
        "* You can attach additional figures (drawings, graphs,...) in a separate PDF file, just make sure to refer to them in your answers.\n",
        "\n",
        "* $\\large\\LaTeX$ <a href=\"https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index\">Cheat-Sheet</a> (to write equations)\n",
        "    * <a href=\"http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf\">Another Cheat-Sheet</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsqSFZG1pGhj"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 1 -Dropout\n",
        "---\n",
        "In this question, we are going to analyze the following idea:\n",
        "\n",
        "**Idea: use Droput regularization as a feature selection mechanism for the input.**\n",
        "\n",
        "To implement the idea, we wish to create a Dropout mask with probability $p_i$ to drop (=zero out) the $i^{th}$ component of the input feature vector, and optimize $p_i$ such that it encourages a deterministic selction of features (i.e., $p_i \\to 0 \\text{ or } 1$).\n",
        "\n",
        "We will analyze the method on the simple case of Linear Regression: $$ \\mathcal{L}(w)=\\frac{1}{2}\\sum_{n=1}^N\\left(y^{(n)} -w^TD^{(n)}x^{(n)} \\right)^2, $$ where $w \\in \\mathbb{R}^d$ is the parameters vector, $x^{(n)}\\in \\mathbb{R}^d$ are the trainin set samples, $y^{(n)} \\in \\mathbb{R}$ are the corresponding labels and $D^{(n)}\\in \\{0, 1 \\}^{d \\times d}$ is the *diagonal* random Dropout mask, where each element is sampled independently according to:\n",
        "$$ D_{ii}^{(n)}=\\frac{1}{1-p_i}\\begin{cases} 1 \\text{ w.p. } 1-p_i \\\\ 0 \\text{ w.p. } p_i \\end{cases} ,$$\n",
        "where $p_i \\in [0,1]$ is the probability to drop (=zero out) the $i^{th}$ component in the input vector, and we denote $p=[p_1, ..., p_d]^T$.\n",
        "\n",
        "1. Find $\\mathbb{E}[D_{ii}^{(n)}]$ and show that $\\mathbb{E}[D_{ii}^{(n)}D_{jj}^{(n)}]=1 + \\delta_{ij}\\frac{p_i}{1-p_i}$.\n",
        "\n",
        "2. Show that the mean cost function $\\bar{\\mathcal{L}}(w,p)$ (the mean is over the *masks*) is $$ \\bar{\\mathcal{L}}(w,p) = \\mathbb{E}[\\mathcal{L}] =\\frac{1}{2}\\sum_{n=1}^N \\left(y^{(n)} - w^Tx^{(n)} \\right)^2 + \\frac{1}{2}\\sum_{i=1}^d \\frac{p_i}{1-p_i}c_iw_i^2, $$ where $c_i = \\sum_{n=1}^N \\left(x_i^{(n)} \\right)^2$. From this section onwards, you can always assume $\\forall i : c_i >0$.\n",
        "\n",
        "3. Briefly explain what is the difference between $\\bar{\\mathcal{L}}(w,p)$ and the standard Linear Regression loss function without Dropout.\n",
        "\n",
        "4. Note that $\\bar{\\mathcal{L}}(w,p)$ is dependent on $p$, but we know that $p_i \\in [0,1]$. Suggest a function $p=f(u)$ such that we can use Gradient Descent without cinstraints on $\\bar{\\mathcal{L}}(w,f(u))$.\n",
        "\n",
        "5. Recall that we want $p_i \\to 1$ for some features and for the rest $p_i \\to 0$. Assume that there exists a *sparse* solution $w_0$ (that includes zeros), and a *dense* (non-sparse) solution $w_*$ such that $$ \\forall n : y^{(n)} = w_0^Tx^{(n)}=w_*^Tx^{(n)}.$$ Does that necessarily mean that we get $w_0$ by reaching the minimum of $\\bar{\\mathcal{L}}(w,p)$ at $w,p$?\n",
        "\n",
        "6. After some experiments, we got an improvement by adding noise to the input and regularization on $p$, and got $$ \\bar{\\mathcal{L}}(w,p) = \\mathbb{E}[\\mathcal{L}] =\\frac{1}{2}\\sum_{n=1}^N \\left(y^{(n)} - w^Tx^{(n)} \\right)^2 + \\frac{1}{2}\\sum_{i=1}^d \\frac{1}{1-p_i}w_i^2\\sum_{n=1}^N\\left(x_i^{(n)}\\right)^2 + \\mu\\sum_{i=1}^d(1-p_i). $$ Show by calculating $\\bar{\\mathcal{L}}(w)=\\min_{p\\in \\mathbb{R}^d}\\bar{\\mathcal{L}}(w,p)$ that we can omit the Dropout and instead add a regularization $R(w)$ directly to the Linear Regression. Calculate the regularization $R(w)$ and explain how it helps in feature selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3byQjMIpvlF"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 2 - Preventing Variance Explosion\n",
        "---\n",
        "This question relates to lectures 8-9 (from slide 7):\n",
        "\n",
        "Find an initializtion scheme such that $$ \\forall l, i,: \\text{(1) } \\mathbb{E}\\left[F_l(u_l)|u_l\\right]=0, \\text{ (2) } Var(u_l[i]) = 1, $$ assuming skip connections: $u_{l+1} = u_l + F_l(u_l)$ with a single skip $F_l(u_l)=W_l\\phi(u_l)+b_l$ and the activation is ReLU: $\\phi(x) = \\text{ReLU}(x) = \\max(0,x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3XytyMSpvlF"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 3 -Batch Normalization\n",
        "---\n",
        "This question relates to lectures 8-9 (from slide 9):\n",
        "\n",
        "Prove that **without** regularization, BatchNorm **scale invariance** for parameters $\\mathbf{w}$ implies:\n",
        "1. $\\nabla \\mathcal{L}(\\mathbf{w})^T\\mathbf{w} = 0$\n",
        "2. And under gradient flow dynamics ($\\dot{\\mathbf{w}} = -\\eta \\nabla \\mathcal{L}(\\mathbf{w})$) this implies (L2) norm conservation: $\\forall t: ||\\mathbf{w}(t)||^2 = C$\n",
        "\n",
        "Hint: see results from the multilayer networks lecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D-14iM7pGhm"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/officel/80/000000/code.png\" style=\"height:50px;display:inline\"> Part 2 - Code Assignments\n",
        "---\n",
        "* You must write your code in this notebook and save it with the output of all of the code cells.\n",
        "* Additional text can be added in Markdown cells.\n",
        "* You can use any other IDE you like (PyCharm, VSCode...) to write/debug your code, but for the submission you must copy it to this notebook, run the code and save the notebook with the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qAoFL5HRpvlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "720b1cdc-29e7-4363-d16d-a655d34b72c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (1.26.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchdata) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchdata) (16.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchdata) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchdata) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.7.0\n"
          ]
        }
      ],
      "source": [
        "# this part uses the Wikitext-2 dataset. To access torchtext datasets, please install `torchdata`:\n",
        "# `pip install torchdata` ir `conda install -c pytorch torchdata` in activated environment\n",
        "# or `!pip install torchdata` on colab.\n",
        "!pip install torchdata\n",
        "# notes:\n",
        "# torch=2.0.0 <-> torchtext 0.15.1\n",
        "# torch=1.13.0 <-> torchtext 0.14.0\n",
        "# torch=1.12.1 <-> torchtext 0.13.1\n",
        "# downgrading torchtext example: !pip install torchtext==0.13.1 --no-deps\n",
        "# torchtext requires the `portalocker` package to download datasets:\n",
        "!pip install portalocker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "914Kv-TmpvlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d7b0d63-845d-41cb-b8b0-9e9f6349fed0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8eea556df0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# imports for the practice (you can add more if you need)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import math\n",
        "from typing import Tuple\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "# torchtext\n",
        "import torchtext\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "seed = 211\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9IcMJSRFpvlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e458c5d1-ecbc-488b-db4d-6b3fdaffe2c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytorch: 2.0.1+cu118, torchtext: 0.15.2+cpu\n"
          ]
        }
      ],
      "source": [
        "print(f'pytorch: {torch.__version__}, torchtext: {torchtext.__version__}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5sXYvmTpvlH"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/bubbles/50/000000/workflow.png\" style=\"height:50px;display:inline\">  Sequence-to-Sequence with Transformers\n",
        "---\n",
        "* In this exercise, you are going to build a language model using PyTroch's Transformer module.\n",
        "* We will work with the **Wikitext-2** dataset: the WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia.\n",
        "* After training, you will be able to generate senetences!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G95lqjirpvlH"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 1  - Loading and Observing the Data\n",
        "---\n",
        "1. Run the following cells that define the functions `batchify` and `data_process` and initialize the tokenizer, vocabulary and the WikiText2 train dataset.\n",
        "2. Create the train, valid and test data using the provided `batchify` function.\n",
        "5. Print the shape of `train_data`, write in a comment the meaning of each dimension (e.g. `# [meaning of dim1, meaning of dim2]`).\n",
        "6. Print the first 20 words of one training sample from `train_data`. Use the vocabulary you built to transfer between tokens to words: `itos = vocab.vocab.get_itos()` will give a \"int to string\" list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XQRDKQa0pvlH"
      },
      "outputs": [],
      "source": [
        "def batchify(data, bsz):\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AIhize8tpvlH"
      },
      "outputs": [],
      "source": [
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "s2o3vjRCpvlH"
      },
      "outputs": [],
      "source": [
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mxE4NypWpvlH"
      },
      "outputs": [],
      "source": [
        "# train_iter was \"consumed\" by the process of building the vocab,\n",
        "# so we have to create it again\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3c1i8FIFpvlH"
      },
      "outputs": [],
      "source": [
        "batch_size = 20\n",
        "eval_batch_size = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RCtVVe5epvlH"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Your Code Here\n",
        "\"\"\"\n",
        "train_data = batchify(train_data, batch_size)\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [meaning of dim1: number of data batches, meaning of dim2: each data batch size]\n",
        "train_data.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLzM_m4Vxd4D",
        "outputId": "b60dd108-0a80-446a-8134-9257fe67438e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([102499, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "itos = vocab.vocab.get_itos()"
      ],
      "metadata": {
        "id": "9OYtxCEvzD3W"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have train_data and itos available\n",
        "train_sample = train_data[0]  # Get the first training sample\n",
        "\n",
        "# Print the first 20 words using the vocabulary (since batch size is 20, it's the same as peinting the first train sample)\n",
        "first_20_words = [itos[token_id] for token_id in train_sample]\n",
        "print(first_20_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBMcBk4-7u73",
        "outputId": "6cdc0a13-4f30-42ab-c7d6-0c236a1817df"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['=', 'all', 'party', 'major', 'modern', 'polygamist', ',', 'present', 'arrangement', 'trying', '<unk>', 'military', 'offering', 'the', 'and', 'or', 'while', 'escaping', 'receive', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV8MWTe2pvlI"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 2  - Preparing the  Data - Separating to Inputs and Targets\n",
        "---\n",
        "* For a language modeling task, the model needs the following words as `Target`.\n",
        "    * For example, for the senetence \"I have a nice dog\", the model will be given \"I have a nice\" as input, and \"have a nice dog\" as the target.\n",
        "* Implement (complete) the function `get_batch(source, i, bptt)`: it generates the input and target sequence for the transformer model. It subdivides the source data into chunks of length `bptt`.\n",
        "    * For example, for `bptt=2` and at `i=0`, the output of `data, target = get_batch(train_data, i=0, bptt=2)`: `data` will be of shape (2, 20), where the batch size is 20 and `target` will be of length 40 (the target for each element is two words, but we flatten `target`).\n",
        "    * Example: for `bptt=2`, and the ABCDEFG... characters as input, our batches will be in the form of: `data=[a, b], target=[b, c]`. For `bptt=3`: `data=[a, b, c], target=[b, c, d]` and so on. This one example is a batch.\n",
        "    * Print a sample from `data` and `target`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mMw9YqcVpvlI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Your Code Here\n",
        "\"\"\"\n",
        "def get_batch(source, i, bptt):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "        bptt: int\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i + seq_len]\n",
        "    target = source[i + 1 : i + seq_len +1].reshape(-1)\n",
        "\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu2YYZUXpvlI"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 3  - Define Hyperparameters and Initialize the Model\n",
        "---\n",
        "* Define the following hyperparameters (`[a, b]` means in the range between `a` and `b`):\n",
        "    * Embedding size: choose from `[200, 250]`\n",
        "    * Number of hidden units: choose from `[200, 250]`\n",
        "    * Number of layers: choose from `[2, 4]`\n",
        "    * Number of attention heads: choose from `[2, 4]`\n",
        "    * Dropout: choose from `[0.0, 0.3]`\n",
        "    * Loss criterion: `nn.CrossEntropyLoss()`\n",
        "    * Optimizer: choose from `[SGD, Adam, RAdam]`\n",
        "    * Learning rate: choose from `[5e-3, 5.0]`\n",
        "    * Learning Scheduler: `torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)` or any scheduler of your choosing.\n",
        "    * Transformer LayerNormalization: `post` (`norm_first=False`) or `pre` (`norm_first=True`).\n",
        "* Intialize an instance of `TransformerModel` (given) and send it to `device`. Note that you need to give it the number of tokens to define the output of the decoder. You should use the number of tokens in the vocabulary. Print the number of tokens,  print **all** the chosen hyper-parameters and print the model (`print(model`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pKWLfjrGpvlI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5, norm_first=False):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout, norm_first=norm_first)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "T-ye0pY_pvlI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa551078-b50f-4606-ad9d-68d23d0cd3e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 1.0000e-02.\n",
            "number of tokens:  28782\n",
            "all hyper parameters:\n",
            "embedding_size 224\n",
            "hidden_units 225\n",
            "num_layers 4\n",
            "num_attention_heads 4\n",
            "dropout 0.1\n",
            "loss_criterion CrossEntropyLoss()\n",
            "optimizer SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    initial_lr: 0.01\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "learning_rate 0.01\n",
            "scheduler <torch.optim.lr_scheduler.StepLR object at 0x7f8e026c5ed0>\n",
            "norm_first True\n",
            "model:\n",
            "\n",
            " TransformerModel(\n",
            "  (pos_encoder): PositionalEncoding(\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (transformer_encoder): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=224, out_features=224, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=224, out_features=225, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=225, out_features=224, bias=True)\n",
            "        (norm1): LayerNorm((224,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((224,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (encoder): Embedding(28782, 224)\n",
            "  (decoder): Linear(in_features=224, out_features=28782, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Your Code Here\n",
        "\"\"\"\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_size = 224  # Choose from [200, 250]\n",
        "hidden_units = 225  # Choose from [200, 250]\n",
        "num_layers = 4  # Choose from [2, 4]\n",
        "num_attention_heads = 4  # Choose from [2, 4]\n",
        "dropout = 0.1  # Choose from [0.0, 0.3]\n",
        "\n",
        "# Loss criterion\n",
        "loss_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Transformer LayerNormalization\n",
        "layer_norm_type = \"pre\"  # Choose from [\"pre\", \"post\"]\n",
        "if layer_norm_type == \"pre\":\n",
        "    norm_first = True\n",
        "elif layer_norm_type == \"post\":\n",
        "    norm_first = False\n",
        "\n",
        "num_tokens = len(vocab)\n",
        "\n",
        "model = TransformerModel(ntoken=num_tokens,\n",
        "                         ninp=embedding_size,\n",
        "                         nhead=num_attention_heads,\n",
        "                         nhid=hidden_units,\n",
        "                         nlayers=num_layers,\n",
        "                         dropout=dropout,\n",
        "                         norm_first=norm_first)\n",
        "\n",
        "# Optimizer\n",
        "optimizer_choice = \"RAdam\"  # Choose from [\"SGD\", \"Adam\", \"RAdam\"]\n",
        "if optimizer_choice == \"SGD\":\n",
        "    optimizer = torch.optim.SGD\n",
        "elif optimizer_choice == \"Adam\":\n",
        "    optimizer = torch.optim.Adam\n",
        "elif optimizer_choice == \"RAdam\":\n",
        "    optimizer = torch.optim.RAdam\n",
        "\n",
        "learning_rate = 1e-2  # Choose from [5e-3, 5.0]\n",
        "optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 3.0, gamma=0.8, verbose=True)  # You can choose a different scheduler if desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "print('number of tokens: ', num_tokens)\n",
        "\n",
        "print('all hyper parameters:', )\n",
        "hp_dict = {'embedding_size': embedding_size,\n",
        "            'hidden_units': hidden_units,\n",
        "            'num_layers': num_layers,\n",
        "            'num_attention_heads': num_attention_heads,\n",
        "            'dropout': dropout,\n",
        "            'loss_criterion': loss_criterion,\n",
        "            'optimizer': optimizer,\n",
        "            'learning_rate': learning_rate,\n",
        "            'scheduler': scheduler,\n",
        "            'norm_first': norm_first}\n",
        "for key, value in hp_dict.items():\n",
        "    print(key, value)\n",
        "\n",
        "print('model:\\n\\n', model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsiunHewpvlI"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 4  - Train and Evaluate the Language Model\n",
        "---\n",
        "* Fill in the missing line in the training code and train the model.\n",
        "* Use `bptt=35`.\n",
        "* Use the provided function to evaluate it on the validatation set (after each epoch) and on test test (after training is done). **Print and plot** the results (loss and perplexity).\n",
        "* If you see that the performance does not improve, go back to Task 3 and re-think you hyper-parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WTL421zMpvlI"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, eval_data, ntokens, criterion):\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i, bptt)\n",
        "            seq_len = data.size(0)\n",
        "            if seq_len != bptt:\n",
        "                src_mask = src_mask[:seq_len, :seq_len]\n",
        "            output = model(data, src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "llREGCRppvlI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Your Code Here\n",
        "\"\"\"\n",
        "def train(model, bptt, ntokens, criterion):\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "    num_batches = len(train_data) // bptt\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i, bptt)\n",
        "        seq_len = data.size(0)\n",
        "        if seq_len != bptt:  # only on last batch\n",
        "            src_mask = src_mask[:seq_len, :seq_len]\n",
        "\n",
        "        output = model(data, src_mask) # complete\n",
        "        output_flat = output.view(-1, ntokens)\n",
        "        loss = criterion(output_flat, targets) # complete\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}|')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNSWnCXmpvlJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Your Code Here\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "epochs = 25 # complete the number of epochs to run\n",
        "best_model = None\n",
        "bptt = 35\n",
        "\n",
        "ntokens =  len(vocab)\n",
        "criterion = loss_criterion\n",
        "\n",
        "performance_list = []\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    # complete: call train() here with appropriate paramteters\n",
        "    train(model, bptt, ntokens, criterion)\n",
        "\n",
        "    val_loss = evaluate(model, val_data, ntokens, criterion)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        print('updated the best model')\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    performance_list.append({'epoch_n': epoch, 'train_loss': evaluate(model, train_data, ntokens, criterion),  'val_loss': val_loss, 'valid_ppl': math.exp(val_loss)})\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "df = pd.DataFrame(performance_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.plot('epoch_n', ['train_loss', 'val_loss'])\n",
        "print('test loss: ', evaluate(best_model, test_data, ntokens, criterion))"
      ],
      "metadata": {
        "id": "nf4CMZf_0Dhb",
        "outputId": "927ad633-f85f-4cfd-ed71-ec72d295ef03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss:  6.2675820839274925\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('config_1_df.csv')\n",
        "torch.save(best_model.state_dict(), 'config_1_model')\n",
        "\n",
        "with open('config_1.txt', 'w') as f:\n",
        "    print('number of tokens: ', num_tokens, file=f)\n",
        "\n",
        "    print('all hyper parameters:', file=f)\n",
        "    for key, value in hp_dict.items():\n",
        "        print(key, value, file=f)\n",
        "\n",
        "    print('model:\\n\\n', model, file=f)"
      ],
      "metadata": {
        "id": "Pwfjld1V4ZAW"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEhFvgoPpvlJ"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 5  - Generate Sentences\n",
        "---\n",
        "Use the following function to generate 3 sentences of length 20, and print them. Do they make sense? (you can compare generated sentences over epochs, to see if some logic is gained during training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LcSPlHUpvlJ"
      },
      "outputs": [],
      "source": [
        "def generate(model, vocab, nwords=100, temp=1.0):\n",
        "    model.eval()\n",
        "    ntokens = len(vocab)\n",
        "    itos = vocab.vocab.get_itos()\n",
        "    model_input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "    words = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(nwords):\n",
        "            output = model(model_input, None)\n",
        "            word_weights = output[-1].squeeze().div(temp).exp().cpu()\n",
        "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "            word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
        "            model_input = torch.cat([model_input, word_tensor], 0)\n",
        "            word = itos[word_idx]\n",
        "            words.append(word)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQ7HGHSKpvlJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Yout code Here\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "epochs = 15 # complete the number of epochs to run\n",
        "best_model = None\n",
        "bptt = 35\n",
        "\n",
        "ntokens =  len(vocab)\n",
        "criterion = loss_criterion\n",
        "\n",
        "performance_list = []\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    # complete: call train() here with appropriate paramteters\n",
        "    train(model, bptt, ntokens, criterion)\n",
        "\n",
        "    val_loss = evaluate(model, val_data, ntokens, criterion)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        print('updated the best model')\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # generate sentences using the model\n",
        "    sentences = []\n",
        "    for i in range(3):\n",
        "      sentences.append(generate(model, vocab, nwords=20))\n",
        "    print(sentences)\n",
        "\n",
        "    performance_list.append({'epoch_n': epoch, 'train_loss': evaluate(model, train_data, ntokens, criterion),  'val_loss': val_loss, 'valid_ppl': math.exp(val_loss), 'generated_sentences': sentences})\n",
        "\n",
        "df_word_generation = pd.DataFrame(performance_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFVVh7Q6pvlJ"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
        "---\n",
        "* Icons made by <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a>\n",
        "* Icons from <a href=\"https://icons8.com/\">Icons8.com</a> - https://icons8.com\n",
        "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}